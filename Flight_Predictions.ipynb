{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Flight Delay Project - Predictions\n",
    "This Notebook is focused on predicting the Arrival Delay of flights.  This is difference between the actual time a flight arrives at the terminal and the time it was scheduled to arrive.\n",
    "\n",
    "The predictions will take place once the plane takes off.\n",
    "\n",
    "I will utilise 4 different machine learning models.\n",
    "\n",
    "Linear Regression:\n",
    "- Scikit-learn's ElasticNet\n",
    "\n",
    "Gradient Boosting:\n",
    "- LightGBM's LGBMRegressor\n",
    "- XGBoost's XGBRegressor\n",
    "- CatBoost's CatBoostRegressor\n",
    "\n",
    "in order to handle the large amount of categorical data in landed_df."
   ],
   "id": "52c3ddbe65dbafc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 1: Preprocessing the Data",
   "id": "a7edc469f6f6e56d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T14:28:52.161131Z",
     "start_time": "2025-05-24T14:28:50.779246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ],
   "id": "86667c531cc07550",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T14:28:52.723951Z",
     "start_time": "2025-05-24T14:28:52.489507Z"
    }
   },
   "cell_type": "code",
   "source": "landed_df = pd.read_pickle(\"landed_flights.pkl\")",
   "id": "91637574e9c80175",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T14:28:52.771236Z",
     "start_time": "2025-05-24T14:28:52.726862Z"
    }
   },
   "cell_type": "code",
   "source": "landed_df.head()",
   "id": "e8b66161dcbc6ca9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   MONTH  DAY  DAY_OF_WEEK AIRLINE FLIGHT_NUMBER TAIL_NUMBER ORIGIN_AIRPORT  \\\n",
       "0      1    1            4      AS            98      N407AS            ANC   \n",
       "1      1    1            4      AA          2336      N3KUAA            LAX   \n",
       "2      1    1            4      US           840      N171US            SFO   \n",
       "3      1    1            4      AA           258      N3HYAA            LAX   \n",
       "4      1    1            4      AS           135      N527AS            SEA   \n",
       "\n",
       "  DESTINATION_AIRPORT SCHEDULED_DEPARTURE      DEPARTURE_TIME  ...  \\\n",
       "0                 SEA 2015-01-01 00:05:00 2014-12-31 23:54:00  ...   \n",
       "1                 PBI 2015-01-01 00:10:00 2015-01-01 00:02:00  ...   \n",
       "2                 CLT 2015-01-01 00:20:00 2015-01-01 00:18:00  ...   \n",
       "3                 MIA 2015-01-01 00:20:00 2015-01-01 00:15:00  ...   \n",
       "4                 ANC 2015-01-01 00:25:00 2015-01-01 00:24:00  ...   \n",
       "\n",
       "   ARRIVAL_DELAY  AIR_SYSTEM_DELAY SECURITY_DELAY  AIRLINE_DELAY  \\\n",
       "0            -22                 0              0              0   \n",
       "1             -9                 0              0              0   \n",
       "2              5                 0              0              0   \n",
       "3             -9                 0              0              0   \n",
       "4            -21                 0              0              0   \n",
       "\n",
       "   LATE_AIRCRAFT_DELAY  WEATHER_DELAY  SCHEDULED_DEPARTURE_HOURS  \\\n",
       "0                    0              0                          0   \n",
       "1                    0              0                          0   \n",
       "2                    0              0                          0   \n",
       "3                    0              0                          0   \n",
       "4                    0              0                          0   \n",
       "\n",
       "  SCHEDULED_DEPARTURE_MINUTES  SCHEDULED_ARRIVAL_HOUR_IN_DESTINATION_TIMEZONE  \\\n",
       "0                           5                                               4   \n",
       "1                          10                                               7   \n",
       "2                          20                                               8   \n",
       "3                          20                                               8   \n",
       "4                          25                                               3   \n",
       "\n",
       "  SCHEDULED_ARRIVAL_MINUTE_IN_DESTINATION_TIMEZONE  \n",
       "0                                               30  \n",
       "1                                               50  \n",
       "2                                                6  \n",
       "3                                                5  \n",
       "4                                               20  \n",
       "\n",
       "[5 rows x 31 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>AIRLINE</th>\n",
       "      <th>FLIGHT_NUMBER</th>\n",
       "      <th>TAIL_NUMBER</th>\n",
       "      <th>ORIGIN_AIRPORT</th>\n",
       "      <th>DESTINATION_AIRPORT</th>\n",
       "      <th>SCHEDULED_DEPARTURE</th>\n",
       "      <th>DEPARTURE_TIME</th>\n",
       "      <th>...</th>\n",
       "      <th>ARRIVAL_DELAY</th>\n",
       "      <th>AIR_SYSTEM_DELAY</th>\n",
       "      <th>SECURITY_DELAY</th>\n",
       "      <th>AIRLINE_DELAY</th>\n",
       "      <th>LATE_AIRCRAFT_DELAY</th>\n",
       "      <th>WEATHER_DELAY</th>\n",
       "      <th>SCHEDULED_DEPARTURE_HOURS</th>\n",
       "      <th>SCHEDULED_DEPARTURE_MINUTES</th>\n",
       "      <th>SCHEDULED_ARRIVAL_HOUR_IN_DESTINATION_TIMEZONE</th>\n",
       "      <th>SCHEDULED_ARRIVAL_MINUTE_IN_DESTINATION_TIMEZONE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>AS</td>\n",
       "      <td>98</td>\n",
       "      <td>N407AS</td>\n",
       "      <td>ANC</td>\n",
       "      <td>SEA</td>\n",
       "      <td>2015-01-01 00:05:00</td>\n",
       "      <td>2014-12-31 23:54:00</td>\n",
       "      <td>...</td>\n",
       "      <td>-22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>AA</td>\n",
       "      <td>2336</td>\n",
       "      <td>N3KUAA</td>\n",
       "      <td>LAX</td>\n",
       "      <td>PBI</td>\n",
       "      <td>2015-01-01 00:10:00</td>\n",
       "      <td>2015-01-01 00:02:00</td>\n",
       "      <td>...</td>\n",
       "      <td>-9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>US</td>\n",
       "      <td>840</td>\n",
       "      <td>N171US</td>\n",
       "      <td>SFO</td>\n",
       "      <td>CLT</td>\n",
       "      <td>2015-01-01 00:20:00</td>\n",
       "      <td>2015-01-01 00:18:00</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>AA</td>\n",
       "      <td>258</td>\n",
       "      <td>N3HYAA</td>\n",
       "      <td>LAX</td>\n",
       "      <td>MIA</td>\n",
       "      <td>2015-01-01 00:20:00</td>\n",
       "      <td>2015-01-01 00:15:00</td>\n",
       "      <td>...</td>\n",
       "      <td>-9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>AS</td>\n",
       "      <td>135</td>\n",
       "      <td>N527AS</td>\n",
       "      <td>SEA</td>\n",
       "      <td>ANC</td>\n",
       "      <td>2015-01-01 00:25:00</td>\n",
       "      <td>2015-01-01 00:24:00</td>\n",
       "      <td>...</td>\n",
       "      <td>-21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T14:28:53.623329Z",
     "start_time": "2025-05-24T14:28:52.866718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Combine hours and minutes for Scheduled Departure.\n",
    "landed_df[\"SCHEDULED_DEPARTURE_HOURS_MINUTES\"] =\\\n",
    "    (landed_df[\"SCHEDULED_DEPARTURE_HOURS\"].astype(\"uint16\") * 60) + landed_df[\"SCHEDULED_DEPARTURE_MINUTES\"]\n",
    "\n",
    "# Combine hours and minutes for Scheduled Arrival.\n",
    "landed_df[\"SCHEDULED_ARRIVAL_HOURS_MINUTES\"] =\\\n",
    "    (landed_df[\"SCHEDULED_ARRIVAL\"].dt.hour * 60) + landed_df[\"SCHEDULED_ARRIVAL\"].dt.minute\n",
    "\n",
    "# Combine hours and minutes for Scheduled Arrival in Destination Timezone.\n",
    "landed_df[\"SCHEDULED_ARRIVAL_IN_DESTINATION_TIMEZONE_HOURS_MINUTES\"] =\\\n",
    "    (landed_df[\"SCHEDULED_ARRIVAL_HOUR_IN_DESTINATION_TIMEZONE\"].astype(\"uint16\") * 60) + landed_df[\"SCHEDULED_ARRIVAL_MINUTE_IN_DESTINATION_TIMEZONE\"]\n",
    "\n",
    "# Drop the Hour columns, as they are replaced by the combined hour and minute columns as a record of the time of the day but keep the minute columns as categorical data.\n",
    "landed_df = landed_df.drop([\"SCHEDULED_DEPARTURE_HOURS\",\"SCHEDULED_ARRIVAL_HOUR_IN_DESTINATION_TIMEZONE\"], axis=1)"
   ],
   "id": "d5d803af88afbee9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T14:28:56.283993Z",
     "start_time": "2025-05-24T14:28:53.686910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a derivative feature for the Origin Airport and Airline combinations.\n",
    "landed_df[\"OG_AIRPORT_AIRLINE\"] = landed_df[\"ORIGIN_AIRPORT\"].astype(\"str\") + \"__\" + landed_df[\"AIRLINE\"].astype(\"str\")\n",
    "\n",
    "# Change dtypes to category for categorical data\n",
    "landed_df = landed_df.astype({\"DAY_OF_WEEK\":\"category\",\n",
    "                              \"MONTH\":\"category\",\n",
    "                              \"DAY\":\"category\",\n",
    "                              \"SCHEDULED_DEPARTURE_MINUTES\":\"category\",\n",
    "                              \"SCHEDULED_ARRIVAL_MINUTE_IN_DESTINATION_TIMEZONE\":\"category\",\n",
    "                              \"OG_AIRPORT_AIRLINE\":\"category\"})"
   ],
   "id": "87a0fd0626a53058",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T14:28:57.249792Z",
     "start_time": "2025-05-24T14:28:56.317822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split my data into categorical and numerical columns.\n",
    "cat_cols = [\"AIRLINE\",\n",
    "            \"ORIGIN_AIRPORT\",\n",
    "            \"DESTINATION_AIRPORT\",\n",
    "            \"DAY_OF_WEEK\",\n",
    "            \"MONTH\",\n",
    "            \"DAY\",\n",
    "            \"SCHEDULED_DEPARTURE_MINUTES\",\n",
    "            \"SCHEDULED_ARRIVAL_MINUTE_IN_DESTINATION_TIMEZONE\",\n",
    "            \"OG_AIRPORT_AIRLINE\"]\n",
    "\n",
    "num_cols = [\"DEPARTURE_DELAY\",\n",
    "            \"TAXI_OUT\",\n",
    "            \"DISTANCE\",\n",
    "            \"SCHEDULED_DEPARTURE_HOURS_MINUTES\",\n",
    "            \"SCHEDULED_ARRIVAL_HOURS_MINUTES\",\n",
    "            \"SCHEDULED_ARRIVAL_IN_DESTINATION_TIMEZONE_HOURS_MINUTES\"]\n",
    "\n",
    "# I am transforming these 3 columns because they denote the time of day that a flight departed and I know from the Exploratory_Data_Analysis Notebook that the relationship between these columns and the Arrival Delay is non-linear.\n",
    "transform_cols = [\"SCHEDULED_DEPARTURE_HOURS_MINUTES\",\n",
    "                  \"SCHEDULED_ARRIVAL_IN_DESTINATION_TIMEZONE_HOURS_MINUTES\",\n",
    "                  \"SCHEDULED_ARRIVAL_HOURS_MINUTES\"]\n",
    "\n",
    "squared_data = landed_df[transform_cols].astype(\"float64\") ** 2\n",
    "squared_data.columns = [col + \"_SQUARED\" for col in transform_cols]\n",
    "\n",
    "cubed_data = landed_df[transform_cols].astype(\"float64\") ** 3\n",
    "cubed_data.columns = [col + \"_CUBED\" for col in transform_cols]\n",
    "\n",
    "# Adding the Transformed Data to landed_df.\n",
    "landed_df = pd.concat([landed_df, squared_data, cubed_data], axis=1)\n",
    "\n",
    "# Adding the transformed columns to num_cols.\n",
    "num_cols = num_cols + squared_data.columns.to_list() + cubed_data.columns.to_list()\n",
    "\n",
    "# Releasing memory.\n",
    "squared_data, cubed_data = None, None\n",
    "landed_df = landed_df.drop([\"FLIGHT_NUMBER\",\n",
    "                            \"TAIL_NUMBER\",\n",
    "                            \"SCHEDULED_DEPARTURE\",\n",
    "                            \"DEPARTURE_TIME\",\n",
    "                            \"WHEELS_OFF\",\n",
    "                            \"ELAPSED_TIME\",\n",
    "                            \"AIR_TIME\",\n",
    "                            \"TAXI_IN\",\n",
    "                            \"WHEELS_ON\",\n",
    "                            \"SCHEDULED_ARRIVAL\",\n",
    "                            \"ARRIVAL_TIME\"], axis=1)\n",
    "\n",
    "# Creating a Dataframe for model comparison.  I will conduct a full model analysis at the end of the Notebook.\n",
    "models = [\"LinearRegression\", \"ElasticNet\", \"LGBMRegressor\", \"XGBRegressor\", \"CatBoostRegressor\"]\n",
    "model_comparison = pd.DataFrame({\"Model\": models,\n",
    "                                 \"Test_r2_score\": 0,\n",
    "                                 \"Train_r2_score\": 0,\n",
    "                                 \"Test_RMSE_score\": 0,\n",
    "                                 \"Train_RMSE_score\": 0}).set_index(\"Model\")"
   ],
   "id": "28d7224ba356bac1",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T14:29:09.822660Z",
     "start_time": "2025-05-24T14:28:57.282938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert categorical features to sparse one-hot encoded format for ElasticNet\n",
    "from scipy import sparse\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=True, drop=\"first\")\n",
    "cat_sparse = encoder.fit_transform(landed_df[cat_cols])\n",
    "\n",
    "num_data = landed_df[num_cols]\n",
    "scaler = StandardScaler(with_mean=False) # Keeping zeros to maintain sparsity and save memory\n",
    "scaled_num_data = scaler.fit_transform(num_data)\n",
    "\n",
    "num_sparse = sparse.csr_matrix(scaled_num_data)\n",
    "data_sparse = sparse.hstack([cat_sparse, num_sparse])"
   ],
   "id": "af49c55aa07cf25a",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "***\n",
    "# Step 2: Machine Learning"
   ],
   "id": "b630ef618826ecd7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T14:30:20.133809Z",
     "start_time": "2025-05-24T14:29:09.872188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Splitting the data.\n",
    "train_sparse_data, test_sparse_data, train_result, test_result = train_test_split(data_sparse, landed_df[\"ARRIVAL_DELAY\"], test_size=0.5, random_state=42)\n",
    "\n",
    "# Defining the model.\n",
    "linear =  LinearRegression()\n",
    "linear.fit(train_sparse_data, train_result)\n",
    "\n",
    "# Testing the model.\n",
    "test_prediction = linear.predict(test_sparse_data)\n",
    "train_prediction = linear.predict(train_sparse_data)\n",
    "\n",
    "# Recording the accuracy of the model.\n",
    "test_r2_score = r2_score(test_result, test_prediction)\n",
    "train_r2_score = r2_score(train_result, train_prediction)\n",
    "test_rmse_score = mean_squared_error(test_result, test_prediction, squared=False)\n",
    "train_rmse_score = mean_squared_error(train_result, train_prediction, squared=False)\n",
    "model_comparison.loc[\"LinearRegression\"] = [test_r2_score, train_r2_score, test_rmse_score, train_rmse_score]\n",
    "\n",
    "print(\"Test r2 Score:\", test_r2_score)\n",
    "print(\"Train r2 Score:\", train_r2_score)\n",
    "print(\"Test RMSE score:\", test_rmse_score)\n",
    "print(\"Train RMSE Score:\", train_rmse_score)"
   ],
   "id": "40ffaadca32dd1e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test r2 Score: 0.9391363054383819\n",
      "Train r2 Score: 0.9393619233810927\n",
      "Test RMSE score: 9.682145083281938\n",
      "Train RMSE Score: 9.676767633158814\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T14:30:20.213296Z",
     "start_time": "2025-05-24T14:30:20.183916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Recovering the feature names from OneHotEncoder\n",
    "cat_features = encoder.get_feature_names_out(cat_cols)\n",
    "\n",
    "pd.DataFrame({\"Feature\": list(cat_features) + num_cols,\n",
    "              \"Weight\": linear.coef_,\n",
    "              \"Abs_Weight\":np.abs(linear.coef_)})\\\n",
    "    .set_index(\"Feature\")\\\n",
    "    .nlargest(150, 'Abs_Weight')"
   ],
   "id": "16f4eb511310cb9f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                               Weight  Abs_Weight\n",
       "Feature                                          \n",
       "DEPARTURE_DELAY             36.773139   36.773139\n",
       "OG_AIRPORT_AIRLINE_MSY__MQ -15.262474   15.262474\n",
       "OG_AIRPORT_AIRLINE_JFK__HA  15.224270   15.224270\n",
       "ORIGIN_AIRPORT_LGA         -14.497642   14.497642\n",
       "OG_AIRPORT_AIRLINE_CHS__AS -14.272149   14.272149\n",
       "...                               ...         ...\n",
       "OG_AIRPORT_AIRLINE_OTZ__AS   3.985705    3.985705\n",
       "OG_AIRPORT_AIRLINE_RDU__AS   3.982987    3.982987\n",
       "OG_AIRPORT_AIRLINE_LIH__UA   3.981480    3.981480\n",
       "OG_AIRPORT_AIRLINE_RDU__OO   3.973663    3.973663\n",
       "OG_AIRPORT_AIRLINE_LAW__MQ   3.956509    3.956509\n",
       "\n",
       "[150 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weight</th>\n",
       "      <th>Abs_Weight</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DEPARTURE_DELAY</th>\n",
       "      <td>36.773139</td>\n",
       "      <td>36.773139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OG_AIRPORT_AIRLINE_MSY__MQ</th>\n",
       "      <td>-15.262474</td>\n",
       "      <td>15.262474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OG_AIRPORT_AIRLINE_JFK__HA</th>\n",
       "      <td>15.224270</td>\n",
       "      <td>15.224270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORIGIN_AIRPORT_LGA</th>\n",
       "      <td>-14.497642</td>\n",
       "      <td>14.497642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OG_AIRPORT_AIRLINE_CHS__AS</th>\n",
       "      <td>-14.272149</td>\n",
       "      <td>14.272149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OG_AIRPORT_AIRLINE_OTZ__AS</th>\n",
       "      <td>3.985705</td>\n",
       "      <td>3.985705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OG_AIRPORT_AIRLINE_RDU__AS</th>\n",
       "      <td>3.982987</td>\n",
       "      <td>3.982987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OG_AIRPORT_AIRLINE_LIH__UA</th>\n",
       "      <td>3.981480</td>\n",
       "      <td>3.981480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OG_AIRPORT_AIRLINE_RDU__OO</th>\n",
       "      <td>3.973663</td>\n",
       "      <td>3.973663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OG_AIRPORT_AIRLINE_LAW__MQ</th>\n",
       "      <td>3.956509</td>\n",
       "      <td>3.956509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Departure Delay is the most important feature which we would expect based off of my data analysis.\n",
    "\n",
    "We can also see that many airport airline combinations are part of the most important features as well as origin and destination airports.  This is also expected as they make up the bulk of the categorical features.\n",
    "\n",
    "Only 2 airlines, Spirit, Frontier, and Hawaiian are included in the top 100 most important features.  Airlines despite having many more flights per airline than flights per -airline combination, are less important features (because there is less variation).\n",
    "\n",
    "We can see the effectiveness of the airport-airline derivative feature right at the top of the Dataframe.  JFK as an Origin Airport has a weight of -11 but the JFK-Hawaiian combination has a weight of 8.  This features allows the model to understand the complex relationships between Airlines and Airports"
   ],
   "id": "2bda8e62221cc5be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "***\n",
    "# ElasticNet Linear Regression\n",
    "\n",
    "Given that large size of the dataframe due to the categorical features that have been one-hot-encoded, I am using ElasticNet because it includes the cost of regularisation in its cost function and so aggressively reduces the dimensionality of the dataset by setting less important features to 0.  Based on my exploratory data analysis, I suspect that many of the airlines, and even the airports will have little importance on the arrival delay and so if ElasticNet identifies this and sets those features to 0 it should help avoid overfitting and reduce the time to compute.\n",
    "\n",
    "### How Coordinate Descent in ElasticNet works:\n",
    "ElasticNet takes the first w value (constant) for the first x value (feature) and optimises it with regards to a loss function.  The loss function incorporates both the Mean Squared Error (MSE) and both L1 and L2 regularisation, in order to both minimise the difference between the predicted value and the actual value, as well as the amount of regularisation that is applied to the w value.  The model doesn't use gradient descent, but coordinate descent instead (in order to include L1 regularisation in its loss function).  In coordinate descent, the model finds the optimal value that minimises the loss function and sets the w value to that optimal value.\n",
    "\n",
    "Regularisation is used to prevent the model from overfitting (learning the training data so well that it can't effectively predict new data).\n",
    "\n",
    "During optimisation, L1 regularisation adjusts the w value, each iteration, to reduce its size.  When the new w value is calculated by finding the importance of the feature in reducing the difference between the predicted value and the actual value, the feature may not be very important and so the w value may be below the threshold for L1 regularisation.  These w values are set to 0 because the feature's importance in reducing the loss function is less than the punishment, in the loss function, of using L1 regularisation on the feature.\n",
    "\n",
    "For features, whose w values are above the threshold, they are pulled towards 0, each iteration by a fixed amount by L1 regularisation.  Then, they are pulled towards 0 by L2 regularisation which does so proportionately to the w values it is adjusting. This means that it prevents the model from relying too much on a few large features.\n",
    "\n",
    "Once the model goes through every single w value, optimising each one, it goes back to the first w value and starts again.\n",
    "\n",
    "This process is repeated until the cost function reaches a minimum or the maximum number of iterations has been reached.\n",
    "\n",
    "ElasticNet can accept sparse data because it acts on each individual feature at a time and ignores \"0\" values."
   ],
   "id": "4dd37589456bccda"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T20:19:07.591059Z",
     "start_time": "2025-05-24T14:30:20.293827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Defining my model, the param grid and the grid search.\n",
    "elastic = ElasticNet()\n",
    "\n",
    "param_grid = {\"alpha\":[0.005, 0.01, 0.02],\n",
    "              \"l1_ratio\":[0.8, 0.9, 1.0]}\n",
    "\n",
    "grid_search = GridSearchCV(elastic,\n",
    "                           param_grid,\n",
    "                           cv=3,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "grid_search.fit(train_sparse_data, train_result)\n",
    "best_elastic = grid_search.best_estimator_\n",
    "\n",
    "# Testing the model.\n",
    "test_prediction = best_elastic.predict(test_sparse_data)\n",
    "train_prediction = best_elastic.predict(train_sparse_data)\n",
    "\n",
    "# Recording the accuracy of the model.\n",
    "test_r2_score = r2_score(test_result, test_prediction)\n",
    "train_r2_score = r2_score(train_result, train_prediction)\n",
    "test_rmse_score = mean_squared_error(test_result, test_prediction, squared=False)\n",
    "train_rmse_score = mean_squared_error(train_result, train_prediction, squared=False)\n",
    "model_comparison.loc[\"ElasticNet\"] = [test_r2_score, train_r2_score, test_rmse_score, train_rmse_score]\n",
    "\n",
    "print(\"Test r2 Score:\", test_r2_score)\n",
    "print(\"Train r2 Score:\", train_r2_score)\n",
    "print(\"Test RMSE score:\", test_rmse_score)\n",
    "print(\"Train RMSE Score:\", train_rmse_score)\n",
    "print(grid_search.best_params_)"
   ],
   "id": "abf48e2c937bbc50",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test r2 Score: 0.9375944698104983\n",
      "Train r2 Score: 0.9377558793680788\n",
      "Test RMSE score: 9.80401504383924\n",
      "Train RMSE Score: 9.804078313326153\n",
      "{'alpha': 0.005, 'l1_ratio': 1.0}\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T20:19:07.963102Z",
     "start_time": "2025-05-24T20:19:07.923380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pd.DataFrame({\"Feature\": list(cat_features) + num_cols,\n",
    "              \"Weight\": best_elastic.coef_,\n",
    "              \"Abs_Weight\":np.abs(best_elastic.coef_)})\\\n",
    "    .set_index(\"Feature\")\\\n",
    "    .nlargest(150, 'Abs_Weight')"
   ],
   "id": "300e8d6485cacf8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                              Weight  Abs_Weight\n",
       "Feature                                                         \n",
       "DEPARTURE_DELAY                            36.775224   36.775224\n",
       "ORIGIN_AIRPORT_LGA                        -13.123136   13.123136\n",
       "ORIGIN_AIRPORT_JFK                        -12.580633   12.580633\n",
       "ORIGIN_AIRPORT_EWR                         -8.538402    8.538402\n",
       "TAXI_OUT                                    8.387059    8.387059\n",
       "...                                              ...         ...\n",
       "DAY_6                                      -0.132796    0.132796\n",
       "OG_AIRPORT_AIRLINE_MKE__WN                 -0.116996    0.116996\n",
       "DAY_14                                     -0.114286    0.114286\n",
       "SCHEDULED_DEPARTURE_HOURS_MINUTES_SQUARED   0.102927    0.102927\n",
       "DESTINATION_AIRPORT_CLT                     0.102030    0.102030\n",
       "\n",
       "[150 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weight</th>\n",
       "      <th>Abs_Weight</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DEPARTURE_DELAY</th>\n",
       "      <td>36.775224</td>\n",
       "      <td>36.775224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORIGIN_AIRPORT_LGA</th>\n",
       "      <td>-13.123136</td>\n",
       "      <td>13.123136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORIGIN_AIRPORT_JFK</th>\n",
       "      <td>-12.580633</td>\n",
       "      <td>12.580633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORIGIN_AIRPORT_EWR</th>\n",
       "      <td>-8.538402</td>\n",
       "      <td>8.538402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <td>8.387059</td>\n",
       "      <td>8.387059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAY_6</th>\n",
       "      <td>-0.132796</td>\n",
       "      <td>0.132796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OG_AIRPORT_AIRLINE_MKE__WN</th>\n",
       "      <td>-0.116996</td>\n",
       "      <td>0.116996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAY_14</th>\n",
       "      <td>-0.114286</td>\n",
       "      <td>0.114286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCHEDULED_DEPARTURE_HOURS_MINUTES_SQUARED</th>\n",
       "      <td>0.102927</td>\n",
       "      <td>0.102927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DESTINATION_AIRPORT_CLT</th>\n",
       "      <td>0.102030</td>\n",
       "      <td>0.102030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As we would expect from ElasticNet, the majority of features are set to 0.\n",
    "\n",
    "Fewer Airport-Airline combinations are used than in Linear Regression.  This is because most Airport-Airline combinations have a low prevalence and so even though they are important when they do occur, they don't occur often enough to be considered important by Elastic Net.\n",
    "\n",
    "We can see that many features such as the Day of the Week and the Months that were not in the top 150 for Linear Regression are given importance by Elastic Net.  This will be because they occur many times in the data and so impact more flights despite having less impact on those flights than other features that occur fewer times but have more impact."
   ],
   "id": "3412efbcba3dde8b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "***\n",
    "# XGBRegressor Gradient Boost\n",
    "\n",
    "I am using 3 different Gradient Boosting ML models.  Gradient Boosting, through its decision trees, can handle complex interactions between the categorical and numerical data.  In a situation where flights leaving JFK in the morning having a larger arrival delay than in the morning, when this is the opposite of the usual pattern seen in my data analysis, a Gradient Boosting model, through its decision trees would be able to handle this interaction which Linear Regression would miss.\n",
    "\n",
    "### Creating the Decision Trees\n",
    "The model first creates an initial prediction by predicting the mean probability of result_train (~0.53) for every single row. It does this by putting the log-odds of result_train and putting that into a sigmoid function. The model will be calculating log-odds\n",
    "\n",
    "The model then finds the residuals (the difference between the predicted probability and the actual result) of each row. The residual is also called the gradient.\n",
    "\n",
    "Then, the model selects a random subset of features (determined by colsample_by_tree), and evaluates every possible way to split the features and selects the split that best maximises the gain function.  To do this the split will:\n",
    "\n",
    "- Maximises the square of the sum of residual values (to group together rows with similar errors).\n",
    "- And minimises the sum of Hessian values (to prioritise correcting rows where the model is confidently incorrect).\n",
    "\n",
    "This gives more weight to predicted probabilities that are very wrong. It increases the number of nodes (from 1 to 2).\n",
    "\n",
    "The process repeats itself on each of the new nodes, splitting them as well, and continues to repeat until:\n",
    "\n",
    "- The \"max_depth\" (number of successive splits) is reached.\n",
    "- The sum of hessian values (of rows) at a node is less than \"min_child_weight\".\n",
    "### Calculating the Probability\n",
    "Once the tree has finished splitting the data, the model goes through each individual leaf and calculates for each row in that leaf a new probability.\n",
    "\n",
    "It first calculates:\n",
    "\n",
    "- The Hessian value: probability * (1 - probability)\n",
    "- The log-odds: log( probability / (1 - probability) )\n",
    "\n",
    "Calculate Delta/Leaf Score:\n",
    "\n",
    "- All of the residual values in the leaf are added together.\n",
    "- All of the hessian values in the leaf are added together.\n",
    "- The sum of the residuals is then divided by: the sum of the hessians + L2 regularisation constant. The L2 regularisation constant is determined by \"reg_lambda\".\n",
    "- This value is then multiplied by -1 (in order to minimise, rather than maximise, the loss function). This value is Delta, and it is stored in the leaf as the Leaf Score.\n",
    "\n",
    "Calculate updated probability for each individual row:\n",
    "\n",
    "- Delta is then multiplied by the \"learning_rate\".\n",
    "- This is added to the log-odds for each individual row. (Each row has its own log-odds but the delta is shared by all rows in that leaf.)\n",
    "- Then the new probability is calculated by putting the log-dds in the sigmoid function.\n",
    "- Finally, the new residual value is calculated by finding the difference between the new probability and the actual result.\n",
    "\n",
    "Then, the model takes the new residual values and moves onto the next tree. The number of trees is determined by \"n_estimators\".\n",
    "\n",
    "Predicting the result\n",
    "Each row is put through the first tree and follows the decision nodes until it makes it to a leaf. Once it is at a leaf, the Leaf Score of that leaf is added to the row's log-odds (which starts at 0).\n",
    "\n",
    "Then each row moves on to the next tree and gets a Leaf Score added to its log-odds, and then the next tree and Leaf Score, and so on. When each row has passed through every single tree, the log-odds of each row is converted to probability through the sigmoid function. If the output of the sigmoid function is above 0.5 the model predicts, for that row, that the Blue team will win, if not, it predicts that the red team will win.\n",
    "\n",
    "### My HyperParameters\n",
    "\n",
    "I will explain some of my hyperparameters that I haven't touched on so far.\n",
    "\n",
    "**tree_method=\"hist\"**\n",
    "\n",
    "This enables binning.  Binning is done natively by LGBMRegressor, which is the next Gradient Boosting model I will use but by setting tree_method to \"hist\" XGBRegressor can engage in Binning too.\n",
    "- This puts features into bins which reduces the number of split points needed to be evaluated at each node.\n",
    "- The Hessians and Gradients are calculated for all the values in each bin and so when a split point is evaluated the model doesn't need to add up the 5 million values again but just the 256 sums of the Hessians and Gradients for each bin.\n",
    "- Bins are re-calculated at each individual node based off of the flights in that node.\n",
    "\n",
    "**enable_categorical=True**\n",
    "\n",
    "This allows the model to accept categorical data without one-hot encoding.\n",
    "\n",
    "**n_jobs=-1**\n",
    "\n",
    "This allows the model to use the maximum amount of computing power.\n",
    "\n",
    "**min_child_weight=500**\n",
    "\n",
    "This stops nodes from splitting and creating a leaf that would have less than 500 flights.  This helps to stop overfitting.  I have chosen a large number of 500 because my dataset is very large and my test size is over 2.5 million.\n",
    "\n",
    "**n_estimators and max_depth**\n",
    "\n",
    "I have chosen a relatively low list of potential n_estimators and a high list for max_depth.  This is because I think there will be many complicated interactions between the features (such as flights of a particular airport at a particular time, at a particular time of year, on a particular airline).  By having a large max_depth it increases the chances that the model can find and learn from these interactions.  The low n_estimators is in order to counteract the high max_depth and prevent overfitting."
   ],
   "id": "9c649c529a3830b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T00:23:00.881523Z",
     "start_time": "2025-05-24T20:19:08.916392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Splitting the data.  The 3 Gradient Boost algorithms I am using process categorical features natively (without the need to do one-hot encoding).\n",
    "train_data, test_data, train_result, test_result = train_test_split(landed_df[cat_cols + num_cols], landed_df[\"ARRIVAL_DELAY\"], test_size=0.5, random_state=42)\n",
    "\n",
    "# Defining my model, the param grid and the grid search.\n",
    "xreg = XGBRegressor(tree_method=\"hist\",\n",
    "                    enable_categorical=True,\n",
    "                    reg_alpha=1.5,\n",
    "                    reg_lambda=1.5,\n",
    "                    colsample_bytree=0.5,\n",
    "                    n_jobs=-1,\n",
    "                    min_child_weight=500)\n",
    "\n",
    "param_grid = {\"n_estimators\": [60, 80, 100, 120],\n",
    "              \"max_depth\": [40, 50, 60],\n",
    "              \"min_child_weight\":[500, 750, 1000],\n",
    "              \"learning_rate\": [0.1, 0.2, 0.3]}\n",
    "\n",
    "grid_search = GridSearchCV(xreg,\n",
    "                           param_grid,\n",
    "                           cv=3,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "grid_search.fit(train_data, train_result)\n",
    "best_xreg = grid_search.best_estimator_\n",
    "\n",
    "# Testing the model.\n",
    "test_prediction = best_xreg.predict(test_data)\n",
    "train_prediction = best_xreg.predict(train_data)\n",
    "\n",
    "# Recording the accuracy of the model.\n",
    "test_r2_score = r2_score(test_result, test_prediction)\n",
    "train_r2_score = r2_score(train_result, train_prediction)\n",
    "test_rmse_score = mean_squared_error(test_result, test_prediction, squared=False)\n",
    "train_rmse_score = mean_squared_error(train_result, train_prediction, squared=False)\n",
    "model_comparison.loc[\"XGBRegressor\"] = [test_r2_score, train_r2_score, test_rmse_score, train_rmse_score]\n",
    "\n",
    "print(\"Test r2 Score:\", test_r2_score)\n",
    "print(\"Train r2 Score:\", train_r2_score)\n",
    "print(\"Test RMSE score:\", test_rmse_score)\n",
    "print(\"Train RMSE Score:\", train_rmse_score)\n",
    "print(grid_search.best_params_)"
   ],
   "id": "c95403ee681dc30d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test r2 Score: 0.901951307989357\n",
      "Train r2 Score: 0.9251044005815546\n",
      "Test RMSE score: 12.28891\n",
      "Train RMSE Score: 10.754391\n",
      "{'learning_rate': 0.2, 'max_depth': 40, 'min_child_weight': 500, 'n_estimators': 120}\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T00:23:01.775010Z",
     "start_time": "2025-05-25T00:23:01.762690Z"
    }
   },
   "cell_type": "code",
   "source": "{'learning_rate': 0.2, 'max_depth': 10, 'n_estimators': 200}",
   "id": "335b3cefcf58cc41",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.2, 'max_depth': 10, 'n_estimators': 200}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T00:23:02.053072Z",
     "start_time": "2025-05-25T00:23:02.004918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "xreg_importance = (pd.Series(best_xreg.get_booster().get_score(importance_type=\"gain\"),\n",
    "                             name=\"Importance\")\n",
    "                   .sort_values(ascending=False))\n",
    "\n",
    "xreg_importance/xreg_importance.sum()"
   ],
   "id": "1ba583ce28ba4eaf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DEPARTURE_DELAY                                                    0.858357\n",
       "TAXI_OUT                                                           0.054472\n",
       "OG_AIRPORT_AIRLINE                                                 0.018379\n",
       "AIRLINE                                                            0.010809\n",
       "DESTINATION_AIRPORT                                                0.007590\n",
       "ORIGIN_AIRPORT                                                     0.007380\n",
       "MONTH                                                              0.005810\n",
       "SCHEDULED_DEPARTURE_HOURS_MINUTES                                  0.004279\n",
       "DAY                                                                0.004198\n",
       "DAY_OF_WEEK                                                        0.003478\n",
       "DISTANCE                                                           0.003339\n",
       "SCHEDULED_DEPARTURE_HOURS_MINUTES_SQUARED                          0.002618\n",
       "SCHEDULED_ARRIVAL_IN_DESTINATION_TIMEZONE_HOURS_MINUTES_SQUARED    0.002578\n",
       "SCHEDULED_ARRIVAL_HOURS_MINUTES_CUBED                              0.002407\n",
       "SCHEDULED_ARRIVAL_HOURS_MINUTES                                    0.002398\n",
       "SCHEDULED_ARRIVAL_MINUTE_IN_DESTINATION_TIMEZONE                   0.002168\n",
       "SCHEDULED_DEPARTURE_HOURS_MINUTES_CUBED                            0.002099\n",
       "SCHEDULED_DEPARTURE_MINUTES                                        0.002098\n",
       "SCHEDULED_ARRIVAL_IN_DESTINATION_TIMEZONE_HOURS_MINUTES_CUBED      0.002014\n",
       "SCHEDULED_ARRIVAL_HOURS_MINUTES_SQUARED                            0.001774\n",
       "SCHEDULED_ARRIVAL_IN_DESTINATION_TIMEZONE_HOURS_MINUTES            0.001752\n",
       "Name: Importance, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Departure Delay, as expected is the most important feature followed by Taxi Out.\n",
    "\n",
    "The least important feature is the day of the week.\n",
    "\n",
    "The Airport Airline combination is the most important categorical feature."
   ],
   "id": "22fafe292b7edb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "***\n",
    "# LGBMRegressor Gradient Boost\n",
    "\n",
    "I am using LGBMRegressor because it has a fast time to compute and is better at dealing with large numbers of categorical features.\n",
    "\n",
    "It is similar to XGBRegressor and has the same Gain function (to decide how to split each node), but it works differently and should be more suitable for this project.\n",
    "\n",
    "This is due to several reasons, but I will describe just 2 of them:\n",
    "\n",
    "**Binning Strategy**\n",
    "- LGBMRegressor sorts the categories by the size of their Gradient to Hessian ratio in order to find categories that are most similar to each other to bin together.\n",
    "\n",
    "**Leaf-wise Growth**\n",
    "- LGBMRegressor also uses Leaf-wise Tree growth.  This is different to XGBRegressor, which splits every leaf on each level before moving onto the next one.\n",
    "- LGBMRegressor does not split every single leaf on a level before moving onto the next leaf and will evaluate every single leaf (on all levels) to decide which is the best split.  This means that it makes fewer low-Gain splits because it prioritises the leaves that maximise the Gain function rather than just prioritising the splits that do so and will reach its max_depth by splitting fewer leaves because it enacts fewer low-Gain splits.\n",
    "\n",
    "LGBMRegressor is, in general, just better at handling large amounts of categorical data than XBGRegressor.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "I am using large values for \"num_leaves\" in order to increase the chances of the model finding complicated interactions between features in the data.  In my preliminary run of the model, I did not have any significant problems with overfitting, and so I have not needed to use parameters such as max_depth, min_data_in_leaf, feature_fraction, lambda_l1, or lambda_l2.  However, I have set min_data_in_leaf to its default value."
   ],
   "id": "1cd376868c3444"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T20:35:32.037287Z",
     "start_time": "2025-05-25T18:46:06.876676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Defining my model, the param grid and the grid search.\n",
    "lreg = LGBMRegressor(objective=\"regression\",\n",
    "                     random_state=42,\n",
    "                     verbose=-1,\n",
    "                     min_data_in_leaf=20)\n",
    "\n",
    "param_grid = {\"n_estimators\": [100, 150, 200, 250],\n",
    "              \"num_leaves\": [300, 350, 400, 450, 500],\n",
    "              \"learning_rate\": [0.1, 0.2, 0.4, 0.8]}\n",
    "\n",
    "grid_search = GridSearchCV(lreg,\n",
    "                           param_grid,\n",
    "                           cv=3,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "grid_search.fit(train_data, train_result)\n",
    "best_lreg = grid_search.best_estimator_\n",
    "\n",
    "# Testing the model.\n",
    "test_prediction = best_lreg.predict(test_data)\n",
    "train_prediction = best_lreg.predict(train_data)\n",
    "\n",
    "# Recording the accuracy of the model.\n",
    "test_r2_score = r2_score(test_result, test_prediction)\n",
    "train_r2_score = r2_score(train_result, train_prediction)\n",
    "test_rmse_score = mean_squared_error(test_result, test_prediction, squared=False)\n",
    "train_rmse_score = mean_squared_error(train_result, train_prediction, squared=False)\n",
    "model_comparison.loc[\"LGBMRegressor\"] = [test_r2_score, train_r2_score, test_rmse_score, train_rmse_score]\n",
    "\n",
    "print(\"Test r2 Score:\", test_r2_score)\n",
    "print(\"Train r2 Score:\", train_r2_score)\n",
    "print(\"Test RMSE score:\", test_rmse_score)\n",
    "print(\"Train RMSE Score:\", train_rmse_score)\n",
    "print(grid_search.best_params_)"
   ],
   "id": "c1afae04970744af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test r2 Score: 0.9527491693476293\n",
      "Train r2 Score: 0.9676530141774808\n",
      "Test RMSE score: 8.530945239022634\n",
      "Train RMSE Score: 7.0676415116117814\n",
      "{'learning_rate': 0.2, 'n_estimators': 250, 'num_leaves': 350}\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T12:30:11.862082Z",
     "start_time": "2025-05-25T12:30:11.806641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Feature Importance\n",
    "lreg_importance = (pd.Series(best_lreg.booster_.feature_importance(importance_type='gain'),\n",
    "                             index=cat_cols + num_cols,\n",
    "                             name=\"Importance\")\n",
    "                   .sort_values(ascending=False))\n",
    "\n",
    "lreg_importance / lreg_importance.sum()"
   ],
   "id": "5b42bc55423a4c0e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DEPARTURE_DELAY                                                    0.918609\n",
       "TAXI_OUT                                                           0.035549\n",
       "OG_AIRPORT_AIRLINE                                                 0.016885\n",
       "DESTINATION_AIRPORT                                                0.006198\n",
       "ORIGIN_AIRPORT                                                     0.005465\n",
       "DAY                                                                0.003450\n",
       "SCHEDULED_ARRIVAL_MINUTE_IN_DESTINATION_TIMEZONE                   0.002824\n",
       "DISTANCE                                                           0.002496\n",
       "MONTH                                                              0.002332\n",
       "AIRLINE                                                            0.001638\n",
       "SCHEDULED_DEPARTURE_MINUTES                                        0.001056\n",
       "SCHEDULED_ARRIVAL_IN_DESTINATION_TIMEZONE_HOURS_MINUTES            0.000999\n",
       "SCHEDULED_DEPARTURE_HOURS_MINUTES                                  0.000979\n",
       "SCHEDULED_ARRIVAL_HOURS_MINUTES                                    0.000970\n",
       "DAY_OF_WEEK                                                        0.000551\n",
       "SCHEDULED_DEPARTURE_HOURS_MINUTES_SQUARED                          0.000000\n",
       "SCHEDULED_ARRIVAL_IN_DESTINATION_TIMEZONE_HOURS_MINUTES_SQUARED    0.000000\n",
       "SCHEDULED_ARRIVAL_HOURS_MINUTES_SQUARED                            0.000000\n",
       "SCHEDULED_DEPARTURE_HOURS_MINUTES_CUBED                            0.000000\n",
       "SCHEDULED_ARRIVAL_IN_DESTINATION_TIMEZONE_HOURS_MINUTES_CUBED      0.000000\n",
       "SCHEDULED_ARRIVAL_HOURS_MINUTES_CUBED                              0.000000\n",
       "Name: Importance, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T12:30:12.111048Z",
     "start_time": "2025-05-25T12:30:12.097058Z"
    }
   },
   "cell_type": "code",
   "source": "# Feature Importance Analysis Here",
   "id": "a505347a2be3f573",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# CatBoostRegressor Gradient Boost\n",
    "\n",
    "I am using CatBoostRegressor as a 3rd Gradient Boosting model in order to compare to LGBRegressor and XGBRegressor.  However, I do not think it will produce better results that LGBRegressor, I would still like to compare the results and feature importance of the models.\n",
    "\n",
    "The reason why I don't think it will have a higher predicted accuracy than LGBRegressor is because the patterns in the data are highly specialised and may be unique for individual airports and airlines.  This is why I set no max_depth for LGBRegressor.\n",
    "\n",
    "When CatBoostRegressor is deciding how to split its nodes, it does not do, as XBGRegressor and LGBRegressor do and split each node individually.  It splits every single node (on the same level) the exact same way, so when it is evaluating a split, it evaluates the Gain function for each individual node and then sums the Gain functions all together.  This prioritises finding patterns that are present across the dataset rather than for the samples in an individual node which both helps prevent overfitting and can cause the model to miss out on real interactions within the data."
   ],
   "id": "721bd29f25fa6d1e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T17:45:38.769760Z",
     "start_time": "2025-05-25T12:30:12.382109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Defining my model, the param grid and the grid search.\n",
    "creg = CatBoostRegressor(random_state=42,\n",
    "                         thread_count=-1,\n",
    "                         verbose=False)\n",
    "\n",
    "param_grid = {\"depth\": [10, 12, 14, 16],\n",
    "              \"iterations\": [60, 80, 100, 120, 140],\n",
    "              \"learning_rate\":[0.1, 0.2, 0.3]}\n",
    "\n",
    "grid_search = GridSearchCV(creg,\n",
    "                           param_grid,\n",
    "                           cv=2,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "grid_search.fit(train_data, train_result, cat_features=cat_cols)\n",
    "best_creg = grid_search.best_estimator_\n",
    "\n",
    "# Testing the model.\n",
    "test_prediction = best_creg.predict(test_data)\n",
    "train_prediction = best_creg.predict(train_data)\n",
    "\n",
    "# Recording the accuracy of the model.\n",
    "test_r2_score = r2_score(test_result, test_prediction)\n",
    "train_r2_score = r2_score(train_result, train_prediction)\n",
    "test_rmse_score = mean_squared_error(test_result, test_prediction, squared=False)\n",
    "train_rmse_score = mean_squared_error(train_result, train_prediction, squared=False)\n",
    "model_comparison.loc[\"CatBoostRegressor\"] = [test_r2_score, train_r2_score, test_rmse_score, train_rmse_score]\n",
    "\n",
    "print(\"Test r2 Score:\", test_r2_score)\n",
    "print(\"Train r2 Score:\", train_r2_score)\n",
    "print(\"Test RMSE score:\", test_rmse_score)\n",
    "print(\"Train RMSE Score:\", train_rmse_score)\n",
    "\n",
    "print(grid_search.best_params_)"
   ],
   "id": "e93ce03072be8f9c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\busin\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "22 fits failed out of a total of 120.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "22 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\busin\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\busin\\miniconda3\\lib\\site-packages\\catboost\\core.py\", line 5873, in fit\n",
      "    return self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline,\n",
      "  File \"C:\\Users\\busin\\miniconda3\\lib\\site-packages\\catboost\\core.py\", line 2410, in _fit\n",
      "    self._train(\n",
      "  File \"C:\\Users\\busin\\miniconda3\\lib\\site-packages\\catboost\\core.py\", line 1790, in _train\n",
      "    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "  File \"_catboost.pyx\", line 5023, in _catboost._CatBoost._train\n",
      "  File \"_catboost.pyx\", line 5072, in _catboost._CatBoost._train\n",
      "_catboost.CatBoostError: bad allocation\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\busin\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [0.93236968 0.93563209 0.93634529 0.93408282 0.93654166 0.9373327\n",
      " 0.9353985  0.93723556 0.93808094 0.93602442 0.9378411  0.93865726\n",
      " 0.93655397 0.9382676  0.93919161 0.93310624 0.93658628 0.93728458\n",
      " 0.93476888 0.93749082 0.93846045 0.93624894 0.93827048 0.93925263\n",
      " 0.93695525 0.93891622 0.93989839 0.93751642 0.93945078 0.94040709\n",
      " 0.93348359 0.93725177 0.93790113 0.93522165 0.93833772 0.93892474\n",
      " 0.93661516 0.93913568 0.93971758 0.93778199 0.9398098  0.94035136\n",
      " 0.93841708 0.94032404 0.94083959        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.94064243        nan 0.94055494        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test r2 Score: 0.9421687451528881\n",
      "Train r2 Score: 0.9475266386685753\n",
      "Test RMSE score: 9.437864509938322\n",
      "Train RMSE Score: 9.001752051097325\n",
      "{'depth': 14, 'iterations': 140, 'learning_rate': 0.3}\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T17:45:39.311095Z",
     "start_time": "2025-05-25T17:45:39.032573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "creg_importance = (pd.Series(best_creg.get_feature_importance(),\n",
    "                            index=cat_cols + num_cols,\n",
    "                            name=\"Importance\")\n",
    "                   .sort_values(ascending=False))\n",
    "\n",
    "creg_importance / creg_importance.sum()"
   ],
   "id": "58925ebb02e087c4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DEPARTURE_DELAY                                                    0.596280\n",
       "TAXI_OUT                                                           0.116930\n",
       "ORIGIN_AIRPORT                                                     0.062287\n",
       "AIRLINE                                                            0.047204\n",
       "DISTANCE                                                           0.044028\n",
       "DESTINATION_AIRPORT                                                0.025829\n",
       "OG_AIRPORT_AIRLINE                                                 0.021349\n",
       "MONTH                                                              0.019464\n",
       "SCHEDULED_DEPARTURE_HOURS_MINUTES_CUBED                            0.007974\n",
       "SCHEDULED_ARRIVAL_HOURS_MINUTES                                    0.006893\n",
       "SCHEDULED_DEPARTURE_HOURS_MINUTES                                  0.006665\n",
       "SCHEDULED_ARRIVAL_HOURS_MINUTES_SQUARED                            0.006612\n",
       "SCHEDULED_DEPARTURE_MINUTES                                        0.006486\n",
       "SCHEDULED_DEPARTURE_HOURS_MINUTES_SQUARED                          0.005976\n",
       "SCHEDULED_ARRIVAL_MINUTE_IN_DESTINATION_TIMEZONE                   0.005821\n",
       "SCHEDULED_ARRIVAL_HOURS_MINUTES_CUBED                              0.005208\n",
       "SCHEDULED_ARRIVAL_IN_DESTINATION_TIMEZONE_HOURS_MINUTES_CUBED      0.004464\n",
       "SCHEDULED_ARRIVAL_IN_DESTINATION_TIMEZONE_HOURS_MINUTES_SQUARED    0.004323\n",
       "SCHEDULED_ARRIVAL_IN_DESTINATION_TIMEZONE_HOURS_MINUTES            0.002439\n",
       "DAY                                                                0.002428\n",
       "DAY_OF_WEEK                                                        0.001341\n",
       "Name: Importance, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Stacking Model",
   "id": "73f46ec830385937"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T17:46:09.583117Z",
     "start_time": "2025-05-25T17:45:39.484371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "meta_model_train_data = np.column_stack([best_lreg.predict(train_data),\n",
    "                                         linear.predict(train_sparse_data)])\n",
    "\n",
    "\n",
    "stacking_model = LinearRegression()\n",
    "\n",
    "stacking_model.fit(meta_model_train_data, train_result)\n",
    "\n",
    "meta_model_test_data = np.column_stack([best_lreg.predict(test_data),\n",
    "                                        linear.predict(test_sparse_data)])\n",
    "\n",
    "\n",
    "test_prediction = stacking_model.predict(meta_model_test_data)\n",
    "train_prediction = stacking_model.predict(meta_model_train_data)\n",
    "\n",
    "test_r2_score = r2_score(test_result, test_prediction)\n",
    "train_r2_score = r2_score(train_result, train_prediction)\n",
    "test_rmse_score = mean_squared_error(test_result, test_prediction, squared=False)\n",
    "train_rmse_score = mean_squared_error(train_result, train_prediction, squared=False)\n",
    "model_comparison.loc[\"Stacking_Model\"] = [test_r2_score, train_r2_score, test_rmse_score, train_rmse_score]\n",
    "\n",
    "print(\"Test r2 Score:\", test_r2_score)\n",
    "print(\"Train r2 Score:\", train_r2_score)\n",
    "print(\"Test RMSE score:\", test_rmse_score)\n",
    "print(\"Train RMSE Score:\", train_rmse_score)"
   ],
   "id": "8137cb7cb068781b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test r2 Score: 0.9496712185106274\n",
      "Train r2 Score: 0.9705395504341796\n",
      "Test RMSE score: 8.804417701209655\n",
      "Train RMSE Score: 6.744927544466131\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T17:46:09.712815Z",
     "start_time": "2025-05-25T17:46:09.698305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pd.DataFrame({\"Base_Model\":[\"LGBMRegressor\", \"Linear Regression\"],\n",
    "              \"Weight\":stacking_model.coef_})"
   ],
   "id": "63eda140afbc936e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          Base_Model    Weight\n",
       "0      LGBMRegressor  1.263697\n",
       "1  Linear Regression -0.263340"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Base_Model</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>1.263697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>-0.263340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Analysis",
   "id": "c38d5e04e4b24a46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T17:46:10.333019Z",
     "start_time": "2025-05-25T17:46:10.317551Z"
    }
   },
   "cell_type": "code",
   "source": "model_comparison.sort_values(by=\"Test_r2_score\", ascending=False)",
   "id": "888e382e0d081a7c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                   Test_r2_score  Train_r2_score  Test_RMSE_score  \\\n",
       "Model                                                               \n",
       "LGBMRegressor           0.952523        0.969182         8.551310   \n",
       "Stacking_Model          0.949671        0.970540         8.804418   \n",
       "CatBoostRegressor       0.942169        0.947527         9.437865   \n",
       "LinearRegression        0.939136        0.939362         9.682145   \n",
       "ElasticNet              0.937594        0.937756         9.804015   \n",
       "XGBRegressor            0.901951        0.925104        12.288910   \n",
       "\n",
       "                   Train_RMSE_score  \n",
       "Model                                \n",
       "LGBMRegressor              6.898595  \n",
       "Stacking_Model             6.744928  \n",
       "CatBoostRegressor          9.001752  \n",
       "LinearRegression           9.676768  \n",
       "ElasticNet                 9.804078  \n",
       "XGBRegressor              10.754391  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_r2_score</th>\n",
       "      <th>Train_r2_score</th>\n",
       "      <th>Test_RMSE_score</th>\n",
       "      <th>Train_RMSE_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LGBMRegressor</th>\n",
       "      <td>0.952523</td>\n",
       "      <td>0.969182</td>\n",
       "      <td>8.551310</td>\n",
       "      <td>6.898595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stacking_Model</th>\n",
       "      <td>0.949671</td>\n",
       "      <td>0.970540</td>\n",
       "      <td>8.804418</td>\n",
       "      <td>6.744928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoostRegressor</th>\n",
       "      <td>0.942169</td>\n",
       "      <td>0.947527</td>\n",
       "      <td>9.437865</td>\n",
       "      <td>9.001752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression</th>\n",
       "      <td>0.939136</td>\n",
       "      <td>0.939362</td>\n",
       "      <td>9.682145</td>\n",
       "      <td>9.676768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ElasticNet</th>\n",
       "      <td>0.937594</td>\n",
       "      <td>0.937756</td>\n",
       "      <td>9.804015</td>\n",
       "      <td>9.804078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBRegressor</th>\n",
       "      <td>0.901951</td>\n",
       "      <td>0.925104</td>\n",
       "      <td>12.288910</td>\n",
       "      <td>10.754391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T17:46:10.735789Z",
     "start_time": "2025-05-25T17:46:10.621537Z"
    }
   },
   "cell_type": "code",
   "source": "landed_df[\"ARRIVAL_DELAY\"].std()",
   "id": "a8596b1be5a31da5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.271297093886396"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Conclusions",
   "id": "9161fd97109b7cc4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T01:45:45.647582Z",
     "start_time": "2025-05-25T01:45:45.632919Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "75707e6f78cd042",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
